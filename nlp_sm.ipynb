{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a63344f-a2f9-4ae9-a361-812e698436af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c2e002-38c8-452d-8052-d88975ad426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd091aa-4491-4f6e-b526-481748b4783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/notebooks/data/2022-10-25_az-lyrics-all.csv')\n",
    "\n",
    "#, nrows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7ae7d4-6988-49f7-91e6-7126a9e37fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>typed_by</th>\n",
       "      <th>url</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Crimey</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>My lil homie caught a\\nMy lil homie caught a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Diamonds</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>I can't wait to put them diamonds on my fist\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Trendset</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>I been waitin' on this moment\\nAnd I been wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>070 Shake</td>\n",
       "      <td>Trust Nobody (S)</td>\n",
       "      <td>Trust Nobody</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/070shake/rm_bside/...</td>\n",
       "      <td>Gave you all the secrets, now you creepin' in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69 Boyz</td>\n",
       "      <td>The Wait is Over</td>\n",
       "      <td>Woof Woof</td>\n",
       "      <td>ddaeubl@usa.net, krzyb@yahoo.com</td>\n",
       "      <td>https://ohhla.com/anonymous/69_boyz/waitover/w...</td>\n",
       "      <td>Better ride\\nYou in tune to the sounds\\nOf the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>Chi-Ali</td>\n",
       "      <td>The Fabulous Chi-Ali</td>\n",
       "      <td>Age Ain't Nothing But a #</td>\n",
       "      <td>scott_rodkey@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/chi_ali/fabulous/a...</td>\n",
       "      <td>Chorus: KRS-One sample\\n\\n\"Girls look soooo go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>Chi-Ali f/ Dove (De La Soul), Dres, Phife, Fas...</td>\n",
       "      <td>The Fabulous Chi-Ali</td>\n",
       "      <td>Let the Horns Blow</td>\n",
       "      <td>scott_rodkey@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/chi_ali/fabulous/h...</td>\n",
       "      <td>[Dres]\\nYo! One for your mother, two for your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>Chi-Ali</td>\n",
       "      <td>The Fabulous Chi-Ali</td>\n",
       "      <td>Funky Lemonade</td>\n",
       "      <td>scott_rodkey@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/chi_ali/fabulous/l...</td>\n",
       "      <td>\"Cool in the shade, drink a little bit of le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17600</th>\n",
       "      <td>Chi-Ali</td>\n",
       "      <td>The Fabulous Chi-Ali</td>\n",
       "      <td>Looped It</td>\n",
       "      <td>scott_rodkey@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/chi_ali/fabulous/l...</td>\n",
       "      <td>This ain't a twinkle twinkle little star\\nBut ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17601</th>\n",
       "      <td>Chi-Ali f/ Dove</td>\n",
       "      <td>The Fabulous Chi-Ali</td>\n",
       "      <td>Roadrunner</td>\n",
       "      <td>scott_rodkey@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/chi_ali/fabulous/r...</td>\n",
       "      <td>Verse 1: Chi-Ali\\n\\nI be the Chi, oh who I be\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17602 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  artist  \\\n",
       "0                                              03 Greedo   \n",
       "1                                              03 Greedo   \n",
       "2                                              03 Greedo   \n",
       "3                                              070 Shake   \n",
       "4                                                69 Boyz   \n",
       "...                                                  ...   \n",
       "17597                                            Chi-Ali   \n",
       "17598  Chi-Ali f/ Dove (De La Soul), Dres, Phife, Fas...   \n",
       "17599                                            Chi-Ali   \n",
       "17600                                            Chi-Ali   \n",
       "17601                                    Chi-Ali f/ Dove   \n",
       "\n",
       "                           album                       song  \\\n",
       "0      First Night Out (Mixtape)                     Crimey   \n",
       "1      First Night Out (Mixtape)                   Diamonds   \n",
       "2      First Night Out (Mixtape)                   Trendset   \n",
       "3               Trust Nobody (S)               Trust Nobody   \n",
       "4               The Wait is Over                  Woof Woof   \n",
       "...                          ...                        ...   \n",
       "17597       The Fabulous Chi-Ali  Age Ain't Nothing But a #   \n",
       "17598       The Fabulous Chi-Ali         Let the Horns Blow   \n",
       "17599       The Fabulous Chi-Ali             Funky Lemonade   \n",
       "17600       The Fabulous Chi-Ali                  Looped It   \n",
       "17601       The Fabulous Chi-Ali                 Roadrunner   \n",
       "\n",
       "                               typed_by  \\\n",
       "0                             AZ Lyrics   \n",
       "1                             AZ Lyrics   \n",
       "2                             AZ Lyrics   \n",
       "3                             AZ Lyrics   \n",
       "4      ddaeubl@usa.net, krzyb@yahoo.com   \n",
       "...                                 ...   \n",
       "17597          scott_rodkey@hotmail.com   \n",
       "17598          scott_rodkey@hotmail.com   \n",
       "17599          scott_rodkey@hotmail.com   \n",
       "17600          scott_rodkey@hotmail.com   \n",
       "17601          scott_rodkey@hotmail.com   \n",
       "\n",
       "                                                     url  \\\n",
       "0      https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "1      https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "2      https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "3      https://ohhla.com/anonymous/070shake/rm_bside/...   \n",
       "4      https://ohhla.com/anonymous/69_boyz/waitover/w...   \n",
       "...                                                  ...   \n",
       "17597  https://ohhla.com/anonymous/chi_ali/fabulous/a...   \n",
       "17598  https://ohhla.com/anonymous/chi_ali/fabulous/h...   \n",
       "17599  https://ohhla.com/anonymous/chi_ali/fabulous/l...   \n",
       "17600  https://ohhla.com/anonymous/chi_ali/fabulous/l...   \n",
       "17601  https://ohhla.com/anonymous/chi_ali/fabulous/r...   \n",
       "\n",
       "                                                   lyric  \n",
       "0      My lil homie caught a\\nMy lil homie caught a h...  \n",
       "1      I can't wait to put them diamonds on my fist\\n...  \n",
       "2      I been waitin' on this moment\\nAnd I been wait...  \n",
       "3      Gave you all the secrets, now you creepin' in ...  \n",
       "4      Better ride\\nYou in tune to the sounds\\nOf the...  \n",
       "...                                                  ...  \n",
       "17597  Chorus: KRS-One sample\\n\\n\"Girls look soooo go...  \n",
       "17598  [Dres]\\nYo! One for your mother, two for your ...  \n",
       "17599    \"Cool in the shade, drink a little bit of le...  \n",
       "17600  This ain't a twinkle twinkle little star\\nBut ...  \n",
       "17601  Verse 1: Chi-Ali\\n\\nI be the Chi, oh who I be\\...  \n",
       "\n",
       "[17602 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e38e128-95c7-4843-8e0c-38e069681fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeting the key variables for training\n",
    "df_lyrics = df[['artist','album','song','lyric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4c23a3-7c9d-4507-83fe-fe5e532082d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning dataset where there are no lyrics in a dataset\n",
    "df_lyrics= df_lyrics[df_lyrics['lyric']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2a4ef3-8bbb-48a3-844a-31c0ddfa54eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17540</td>\n",
       "      <td>17308</td>\n",
       "      <td>17468</td>\n",
       "      <td>17601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6899</td>\n",
       "      <td>4818</td>\n",
       "      <td>15457</td>\n",
       "      <td>17233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>311</td>\n",
       "      <td>Zu-Chronicles, Vol. 6: King Monk</td>\n",
       "      <td>Intro</td>\n",
       "      <td>[Hook: Ol' Dirty Bastard]\\nGet prepared for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>207</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist                             album   song  \\\n",
       "count   17540                             17308  17468   \n",
       "unique   6899                              4818  15457   \n",
       "top       311  Zu-Chronicles, Vol. 6: King Monk  Intro   \n",
       "freq      207                                38     36   \n",
       "\n",
       "                                                    lyric  \n",
       "count                                               17601  \n",
       "unique                                              17233  \n",
       "top     [Hook: Ol' Dirty Bastard]\\nGet prepared for th...  \n",
       "freq                                                    4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review how many rows have been deleted\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0beda46-0b4e-4237-b81f-9463282a970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "df_lyrics= df_lyrics[df_lyrics['lyric']!=None].drop_duplicates(subset=['lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f75896b-44a8-4d50-a8d0-fdb5b134742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with null values\n",
    "df_lyrics = df_lyrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d578a886-67cf-4bbb-b04b-910e46bcec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16778</td>\n",
       "      <td>16778</td>\n",
       "      <td>16778</td>\n",
       "      <td>16778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6619</td>\n",
       "      <td>4717</td>\n",
       "      <td>15204</td>\n",
       "      <td>16778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>311</td>\n",
       "      <td>Thought for Food: Vol. 1 &amp; 2</td>\n",
       "      <td>Intro</td>\n",
       "      <td>My lil homie caught a\\nMy lil homie caught a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>207</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist                         album   song  \\\n",
       "count   16778                         16778  16778   \n",
       "unique   6619                          4717  15204   \n",
       "top       311  Thought for Food: Vol. 1 & 2  Intro   \n",
       "freq      207                            30     36   \n",
       "\n",
       "                                                    lyric  \n",
       "count                                               16778  \n",
       "unique                                              16778  \n",
       "top     My lil homie caught a\\nMy lil homie caught a h...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see how many rows have been deleted after cleaning\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a189166b-6341-4fb2-aa56-56cc3a07a21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist                                             Chronixx\n",
       "album                                 Dread & Terrible (EP)\n",
       "song                                           Eternal Fire\n",
       "lyric     Yes I yah\\nSelassie I Gimmi dem remedy the fir...\n",
       "Name: 578, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics.iloc[564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b21fd7-8b58-4f20-8c57-e9bc8b1cca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I look at where I'm coming from\\nI know I'm blessed, and I close my eyes and smile\\nSometimes I feel like\\nThe richest man in Babylon\\nAnd I've done my best\\nSo everything's alright inside\\n\\nOh every morning, Oh every morning\\nI rise, I stare at the sun\\nI know it is a blessing\\nSo when the evening comes I\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can, yeah\\nMy two hands in the air as far as I can\\n\\nOh every morning\\nI rise, I stare at the sun\\nI know it is a blessing, yeah\\nLift up my eyes to the hills I'm blessed oh man\\nOoh\\n\\nSometimes I'm lost and far from home\\nBut I'll find my way\\nFollow my heart and know I'll be found, someday\\nWhile I'm lost I learn and live\\nWhat do you say?\\nLet's have a good time until that day\\n\\nOh every morning, oh every morning\\nI rise, I stare at the sun\\nI know it is a blessing\\nSo when the evening comes, I\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh every morning\\nI rise, I stare at the sun\\nI know it is a blessing, yeah\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\n\\nWith my two hands in the air as far as I can\\nI can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics[\"lyric\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d413b0f-2969-43d9-9547-1b44b122cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source - https://huggingface.co/docs/datasets/loading#pandas-dataframe\n",
    "#Source - https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/loading_methods#datasets.load_dataset.split\n",
    "#maybe we need to split the data source during loading\n",
    "dataset = Dataset.from_pandas(df_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e63d4a9-4358-4b6e-a735-6e5c0adb12bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'album': Value(dtype='string', id=None),\n",
       " 'song': Value(dtype='string', id=None),\n",
       " 'lyric': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "921ddefe-eb9d-4c19-b800-b66cde5e27cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'album': Value(dtype='string', id=None),\n",
       " 'lyric': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"song\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d9821ab-bee7-4d28-aeff-b61753659114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album': Value(dtype='string', id=None),\n",
       " 'lyric': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"artist\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b64da73-038e-46f0-b83c-681761be754b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album': Value(dtype='string', id=None),\n",
       " 'lyric': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "476c7e0d-a6cb-43d7-889f-312283097e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lyric': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"album\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaee7c0-ba32-4715-8041-43ca227ed81d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0fe51b5-c30c-4290-988b-101f8467bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 14\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 106, 128, 128, 128, 128, 128, 87]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "context_length = 128\n",
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = tokenizer(\n",
    "    dataset[\"lyric\"][:2],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdbc6e-fd3f-4fdd-ae1c-7519e3f6a1c1",
   "metadata": {},
   "source": [
    "# tokenize def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4032202-f9df-4d32-a3c8-dc2dbc44f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f0d07c4dd30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803b3f6ea2f046fe8b60746a4fcab41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 91170\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"lyric\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=[\"lyric\"])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8172a129-2254-4ddb-8837-bfce0cbdc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_datasets[i] for i in range(8)]\n",
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e316ba55-d4c9-47a1-b29c-880f2556581e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3666,\n",
       "  9529,\n",
       "  22009,\n",
       "  355,\n",
       "  314,\n",
       "  4829,\n",
       "  606,\n",
       "  357,\n",
       "  5633,\n",
       "  1267,\n",
       "  795,\n",
       "  284,\n",
       "  262,\n",
       "  4405,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  1354,\n",
       "  1192,\n",
       "  259,\n",
       "  11,\n",
       "  661,\n",
       "  288,\n",
       "  1192,\n",
       "  259,\n",
       "  287,\n",
       "  262,\n",
       "  4675,\n",
       "  198,\n",
       "  2514,\n",
       "  616,\n",
       "  13026,\n",
       "  11,\n",
       "  517,\n",
       "  10787,\n",
       "  2456,\n",
       "  422,\n",
       "  262,\n",
       "  38290,\n",
       "  530,\n",
       "  198,\n",
       "  1639,\n",
       "  2936,\n",
       "  262,\n",
       "  1245,\n",
       "  287,\n",
       "  262,\n",
       "  1613,\n",
       "  290,\n",
       "  345,\n",
       "  821,\n",
       "  991,\n",
       "  35519,\n",
       "  198,\n",
       "  3123,\n",
       "  20637,\n",
       "  345,\n",
       "  11316,\n",
       "  287,\n",
       "  262,\n",
       "  8977,\n",
       "  318,\n",
       "  257,\n",
       "  1276,\n",
       "  329,\n",
       "  502,\n",
       "  198,\n",
       "  50,\n",
       "  6548,\n",
       "  345,\n",
       "  25573,\n",
       "  82,\n",
       "  1392,\n",
       "  1088,\n",
       "  290,\n",
       "  2112,\n",
       "  262,\n",
       "  402,\n",
       "  198,\n",
       "  42337,\n",
       "  259,\n",
       "  543,\n",
       "  1445,\n",
       "  314,\n",
       "  1244,\n",
       "  787,\n",
       "  1306,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  8931,\n",
       "  259,\n",
       "  616,\n",
       "  9529,\n",
       "  22009,\n",
       "  523,\n",
       "  314,\n",
       "  12080,\n",
       "  284,\n",
       "  262,\n",
       "  40167,\n",
       "  198,\n",
       "  5195,\n",
       "  815,\n",
       "  314,\n",
       "  4043,\n",
       "  329,\n",
       "  257,\n",
       "  3128,\n",
       "  351,\n",
       "  22701,\n",
       "  30,\n",
       "  198,\n",
       "  40,\n",
       "  1183,\n",
       "  2298,\n",
       "  262,\n",
       "  640,\n",
       "  11,\n",
       "  262,\n",
       "  1295,\n",
       "  326,\n",
       "  338,\n",
       "  1266,\n",
       "  329,\n",
       "  502,\n",
       "  198,\n",
       "  3987]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[564]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680dd90-6f09-4f40-938a-2520d5631fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c24778c4-cf4d-4c8f-a065-1d2dd6fa343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    checkpoint,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247179c7-3d8f-410f-ad78-02129ca26544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d72eb9-24af-4033-8cc4-ae7ba21ff44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea811e0e-1124-48d9-b8b2-9c47c052f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5e0b546-ab5a-4a06-b4fc-b2626dd3a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0c26-0d62-4999-b03d-bb9cf8acdeb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# login into HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d7d3a46-00cd-4a04-ab65-17a742f36400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9977e625ea47f9bdc6437ec5f39d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29811cef-d89a-4d48-8bd5-b83ef94fcfe1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# split tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce960e06-0355-4770-9bdc-d343d81ad3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds_train, ds_test = train_test_split(tokenized_datasets, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4885bcf8-d1ec-4e00-9745-152b63815712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.9\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c747d532-3a30-4851-8672-594626edbefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.3.0).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 134 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ac34b-1c35-46d4-a895-8ea226af729f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Aruguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "897110f2-ff04-4d44-b4fc-dd9b431fffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/notebooks/hiphop-ds is already a clone of https://huggingface.co/Bbrown44/hiphop-ds. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"hiphop-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc709a7-1294-4de1-901c-fa7f98364757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:1625\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1624\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1626\u001b[0m \n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1629\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe4d1a-2124-4673-afc8-3aa82a3aa073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

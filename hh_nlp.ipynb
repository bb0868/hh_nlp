{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfa8e4c-ad50-4e22-abc6-0090cc323495",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea4aba4-de39-4770-812a-0ce35b4738d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source https://huggingface.co/course/chapter3/2?fw=pt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754d0820-ec68-42e2-9a59-4d7ad1857cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = load_dataset('csv', data_files='/notebooks/data/2022-10-25_az-lyrics-all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e7bf56-d376-4b81-ba8e-9bd0dbb73ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/notebooks/data/2022-10-25_az-lyrics-all.csv', nrows=1000)\n",
    "\n",
    "#, nrows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a473a-6247-42a0-80ee-c297c40a3036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>typed_by</th>\n",
       "      <th>url</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Crimey</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>My lil homie caught a\\nMy lil homie caught a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Diamonds</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>I can't wait to put them diamonds on my fist\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03 Greedo</td>\n",
       "      <td>First Night Out (Mixtape)</td>\n",
       "      <td>Trendset</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/03greedo/firstout/...</td>\n",
       "      <td>I been waitin' on this moment\\nAnd I been wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>070 Shake</td>\n",
       "      <td>Trust Nobody (S)</td>\n",
       "      <td>Trust Nobody</td>\n",
       "      <td>AZ Lyrics</td>\n",
       "      <td>https://ohhla.com/anonymous/070shake/rm_bside/...</td>\n",
       "      <td>Gave you all the secrets, now you creepin' in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69 Boyz</td>\n",
       "      <td>The Wait is Over</td>\n",
       "      <td>Woof Woof</td>\n",
       "      <td>ddaeubl@usa.net, krzyb@yahoo.com</td>\n",
       "      <td>https://ohhla.com/anonymous/69_boyz/waitover/w...</td>\n",
       "      <td>Better ride\\nYou in tune to the sounds\\nOf the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>C-Murder f/ Goodie MOb</td>\n",
       "      <td>Bossalinie</td>\n",
       "      <td>Where We Wanna</td>\n",
       "      <td>JCIzWhoIBe@aol.com</td>\n",
       "      <td>https://ohhla.com/anonymous/c_murder/bossa/we_...</td>\n",
       "      <td>[T-Mo]\\nTell it.\\nTell it.\\nLet em know.\\nHa h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>C-Murder f/ Afficial</td>\n",
       "      <td>C-P-3.com</td>\n",
       "      <td>Criminal Minded</td>\n",
       "      <td>Lil Hustle</td>\n",
       "      <td>https://ohhla.com/anonymous/c_murder/cp3_com/c...</td>\n",
       "      <td>(Intro)\\nI'm a criminal, he's a criminal\\nShe'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>C-Murder</td>\n",
       "      <td>C-P-3.com</td>\n",
       "      <td>Don't Matter</td>\n",
       "      <td>NJShortie205711@hotmail.com</td>\n",
       "      <td>https://ohhla.com/anonymous/c_murder/cp3_com/d...</td>\n",
       "      <td>No matter how much you hate me\\nI'ma always be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>C-Murder f/Slay Sean, Black Felon, and Traci</td>\n",
       "      <td>C-P-3.Com</td>\n",
       "      <td>Do You Wanna Ride</td>\n",
       "      <td>rodog03@yahoo.com</td>\n",
       "      <td>https://ohhla.com/anonymous/c_murder/cp3_com/d...</td>\n",
       "      <td>[Slay Sean]\\nHow many wanna play now? It get d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>C-Murder f/ Mia X, Ms. Peaches &amp; Traci</td>\n",
       "      <td>C-P-3.com</td>\n",
       "      <td>Down For My B's</td>\n",
       "      <td>Lil Hustle</td>\n",
       "      <td>https://ohhla.com/anonymous/c_murder/cp3_com/f...</td>\n",
       "      <td>(C-Murder talking)\\nFuck them bitches\\nLadies,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           artist                      album  \\\n",
       "0                                       03 Greedo  First Night Out (Mixtape)   \n",
       "1                                       03 Greedo  First Night Out (Mixtape)   \n",
       "2                                       03 Greedo  First Night Out (Mixtape)   \n",
       "3                                       070 Shake           Trust Nobody (S)   \n",
       "4                                         69 Boyz           The Wait is Over   \n",
       "..                                            ...                        ...   \n",
       "995                        C-Murder f/ Goodie MOb                 Bossalinie   \n",
       "996                          C-Murder f/ Afficial                  C-P-3.com   \n",
       "997                                      C-Murder                  C-P-3.com   \n",
       "998  C-Murder f/Slay Sean, Black Felon, and Traci                  C-P-3.Com   \n",
       "999        C-Murder f/ Mia X, Ms. Peaches & Traci                  C-P-3.com   \n",
       "\n",
       "                  song                          typed_by  \\\n",
       "0               Crimey                         AZ Lyrics   \n",
       "1             Diamonds                         AZ Lyrics   \n",
       "2             Trendset                         AZ Lyrics   \n",
       "3         Trust Nobody                         AZ Lyrics   \n",
       "4            Woof Woof  ddaeubl@usa.net, krzyb@yahoo.com   \n",
       "..                 ...                               ...   \n",
       "995     Where We Wanna                JCIzWhoIBe@aol.com   \n",
       "996    Criminal Minded                        Lil Hustle   \n",
       "997       Don't Matter       NJShortie205711@hotmail.com   \n",
       "998  Do You Wanna Ride                 rodog03@yahoo.com   \n",
       "999    Down For My B's                        Lil Hustle   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "1    https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "2    https://ohhla.com/anonymous/03greedo/firstout/...   \n",
       "3    https://ohhla.com/anonymous/070shake/rm_bside/...   \n",
       "4    https://ohhla.com/anonymous/69_boyz/waitover/w...   \n",
       "..                                                 ...   \n",
       "995  https://ohhla.com/anonymous/c_murder/bossa/we_...   \n",
       "996  https://ohhla.com/anonymous/c_murder/cp3_com/c...   \n",
       "997  https://ohhla.com/anonymous/c_murder/cp3_com/d...   \n",
       "998  https://ohhla.com/anonymous/c_murder/cp3_com/d...   \n",
       "999  https://ohhla.com/anonymous/c_murder/cp3_com/f...   \n",
       "\n",
       "                                                 lyric  \n",
       "0    My lil homie caught a\\nMy lil homie caught a h...  \n",
       "1    I can't wait to put them diamonds on my fist\\n...  \n",
       "2    I been waitin' on this moment\\nAnd I been wait...  \n",
       "3    Gave you all the secrets, now you creepin' in ...  \n",
       "4    Better ride\\nYou in tune to the sounds\\nOf the...  \n",
       "..                                                 ...  \n",
       "995  [T-Mo]\\nTell it.\\nTell it.\\nLet em know.\\nHa h...  \n",
       "996  (Intro)\\nI'm a criminal, he's a criminal\\nShe'...  \n",
       "997  No matter how much you hate me\\nI'ma always be...  \n",
       "998  [Slay Sean]\\nHow many wanna play now? It get d...  \n",
       "999  (C-Murder talking)\\nFuck them bitches\\nLadies,...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97c2361-3ba7-4ff8-b3eb-2865f37375e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeting the key variables for training\n",
    "df_lyrics = df[['artist','album','song','lyric']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4ca5d1-c8a5-4833-a190-97fba035025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning dataset where there are no lyrics in a dataset\n",
    "df_lyrics= df_lyrics[df_lyrics['lyric']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b956637c-27cc-4e62-841e-13402c832d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>998</td>\n",
       "      <td>987</td>\n",
       "      <td>995</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>421</td>\n",
       "      <td>253</td>\n",
       "      <td>980</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Chris Webby</td>\n",
       "      <td>RICANstruction: The Black Rosary</td>\n",
       "      <td>Little Man</td>\n",
       "      <td>Rocking for the kiddies\\nTo do shows for rows,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                             album        song  \\\n",
       "count           998                               987         995   \n",
       "unique          421                               253         980   \n",
       "top     Chris Webby  RICANstruction: The Black Rosary  Little Man   \n",
       "freq             90                                28           2   \n",
       "\n",
       "                                                    lyric  \n",
       "count                                                1000  \n",
       "unique                                                997  \n",
       "top     Rocking for the kiddies\\nTo do shows for rows,...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review how many rows have been deleted\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab25bcf-6294-42e2-ba13-c5fee49d6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "df_lyrics= df_lyrics[df_lyrics['lyric']!=None].drop_duplicates(subset=['lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "369e4f7e-7fa0-4e5c-aac1-f4206891d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with null values\n",
    "df_lyrics = df_lyrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dc774a-5807-4829-ab27-aaafd5393ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>406</td>\n",
       "      <td>247</td>\n",
       "      <td>966</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Chris Webby</td>\n",
       "      <td>RICANstruction: The Black Rosary</td>\n",
       "      <td>Trouble</td>\n",
       "      <td>My lil homie caught a\\nMy lil homie caught a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                             album     song  \\\n",
       "count           978                               978      978   \n",
       "unique          406                               247      966   \n",
       "top     Chris Webby  RICANstruction: The Black Rosary  Trouble   \n",
       "freq             90                                28        2   \n",
       "\n",
       "                                                    lyric  \n",
       "count                                                 978  \n",
       "unique                                                978  \n",
       "top     My lil homie caught a\\nMy lil homie caught a h...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see how many rows have been deleted after cleaning\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdb6865-fab5-4e73-b153-68f0e95a69b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist                                             Chronixx\n",
       "album                                 Dread & Terrible (EP)\n",
       "song                                           Eternal Fire\n",
       "lyric     Yes I yah\\nSelassie I Gimmi dem remedy the fir...\n",
       "Name: 578, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics.iloc[564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55df0e73-3aa0-4caf-a634-f854eb447b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I look at where I'm coming from\\nI know I'm blessed, and I close my eyes and smile\\nSometimes I feel like\\nThe richest man in Babylon\\nAnd I've done my best\\nSo everything's alright inside\\n\\nOh every morning, Oh every morning\\nI rise, I stare at the sun\\nI know it is a blessing\\nSo when the evening comes I\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can, yeah\\nMy two hands in the air as far as I can\\n\\nOh every morning\\nI rise, I stare at the sun\\nI know it is a blessing, yeah\\nLift up my eyes to the hills I'm blessed oh man\\nOoh\\n\\nSometimes I'm lost and far from home\\nBut I'll find my way\\nFollow my heart and know I'll be found, someday\\nWhile I'm lost I learn and live\\nWhat do you say?\\nLet's have a good time until that day\\n\\nOh every morning, oh every morning\\nI rise, I stare at the sun\\nI know it is a blessing\\nSo when the evening comes, I\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh every morning\\nI rise, I stare at the sun\\nI know it is a blessing, yeah\\nLift up my eyes to the hills I'm blessed, oh man\\nWith my two hands in the air as far as I can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\n\\nWith my two hands in the air as far as I can\\nI can\\nAs far as I can\\nI can\\nMy two hands in the air as far as I can\\n\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh oh\\nOh oh oh oh oh oh oh\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics[\"lyric\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9796cd4c-7c89-4174-9fe3-0728a6b6e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source - https://huggingface.co/docs/datasets/loading#pandas-dataframe\n",
    "#Source - https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/loading_methods#datasets.load_dataset.split\n",
    "#maybe we need to split the data source during loading\n",
    "dataset = Dataset.from_pandas(df_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667a78c-e103-430f-94db-c762ce0bc18a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203d19ec-05c6-484e-b1c5-1245dfd4e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9d3af91-23db-4172-bec0-74422dbdc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'album': Value(dtype='string', id=None),\n",
       " 'song': Value(dtype='string', id=None),\n",
       " 'lyric': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12ce2b94-3f31-4c5b-80e1-a388d751e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.3.2)\n",
      "Requirement already satisfied: dill<0.3.5 in /usr/local/lib/python3.9/dist-packages (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.23.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#$ pip3 install dill\n",
    "#source https://github.com/huggingface/datasets/issues/4506#issuecomment-1157417219\n",
    "!pip install datasets \"dill<0.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95505e5d-699e-4b3a-a1c9-e939a282a673",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50130ae8-e0ed-43d3-b66b-de15dcb97715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source - using tutorial https://huggingface.co/docs/transformers/training\n",
    "#Source - tokenizer with targeted model https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c0b3e-c34e-44ec-9b4b-d9b48bf78429",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa766478-bde1-40b2-b4b2-d9a102173cf2",
   "metadata": {},
   "source": [
    "tokenizer.pad_token_id\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa38ddf8-7f6d-4d51-bb58-56c5f35463de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://github.com/huggingface/transformers/issues/2630#issuecomment-1290809338\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "592347dd-2821-401c-93e3-1356489264b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78a97ce5fc44035ab0ddfbb849f608b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['artist', 'album', 'song', 'lyric', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 978\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Source - using tutorial https://huggingface.co/docs/transformers/training\n",
    "#first have to tokenize custom dataset\n",
    "#source for below quote: https://huggingface.co/course/chapter3/2?fw=pt\n",
    "#\"Note that we’ve left the padding argument out in our tokenization function for now. \n",
    "#This is because padding all the samples to the maximum length is not efficient: \n",
    "#it’s better to pad the samples when we’re building a batch, as then we only need to pad to the \n",
    "#maximum length in that batch, and not the maximum length in the entire dataset. \n",
    "#This can save a lot of time and processing power when the inputs have very variable lengths!\"\n",
    "\n",
    "\n",
    "#padding and trucation concepts https://huggingface.co/docs/transformers/v4.25.1/en/pad_truncation\n",
    "def tokenize_function(element):\n",
    "    return tokenizer(element[\"lyric\"], truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "##padding=\"max_length\",\n",
    "#max_length=context_length,\n",
    "#we took out the padding because we will use dynamic padding later which is more efficient\n",
    "#padding=\"longest\",\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2471f7d-3f39-48ca-840d-ceb77c63ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 2048\n",
      "Input IDs length: (2048,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "outputs = tokenized_datasets[1]\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input IDs length: {np.array(outputs['input_ids']).shape}\")\n",
    "#print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "#print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b39cba3-51e9-4753-b690-5fbe065a0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://huggingface.co/course/chapter7/6?fw=pt#initializing-a-new-model\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    checkpoint,\n",
    "    vocab_size=len(tokenizer),\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "349a9172-4ee9-4a5b-acc7-e0e10d376d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist', 'album', 'song', 'lyric', '__index_level_0__', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3ef7a6-c7ba-45ef-b057-fc43cbe5b68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"attention_mask\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52293d41-94ca-4d20-b515-85f8db4b3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets[\"input_ids\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6c32b0-ef93-48ba-aa72-a0b9e2c802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://huggingface.co/docs/transformers/training\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010f8d80-f29c-444b-b329-fa69467cad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://huggingface.co/course/chapter7/6?fw=pt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46eabe26-d1e8-4145-9d11-689f9253b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step is to define model\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5bde73-b7cd-4e99-a4b0-3200b3b21ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb72bb14-3a8f-4c83-950d-589810b2e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1\n",
    "#from transformers import TrainingArguments\n",
    "#training_args = TrainingArguments(\"tokenized_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d545a6d-9fca-419f-bb8d-72c9a91a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2 customize all the training arguments\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "   \"tokenized_train\",\n",
    "   per_device_train_batch_size=3,\n",
    "   per_device_eval_batch_size=3,\n",
    "   num_train_epochs=3,\n",
    "   learning_rate=2e-5,\n",
    "   weight_decay=0.01,\n",
    "    fp16=True\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59776d-3f09-4983-926f-e4d0ab545eb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# clean tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fe1bd9e-e213-414d-991e-04e97d5d6698",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (800526342.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [30]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"__index_level_0__\"])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"song\", \"album\", \"artist\", \"lyric\", \n",
    "                                                        \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db459d87-e778-46e9-a3d9-e6c9a3ef9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba2519-f74d-463f-96df-5d605ae09e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source https://github.com/huggingface/transformers/issues/9636#issuecomment-761571219\n",
    "#following code gets error becase dataset has not lable variable - Keyerror: 5\n",
    "\n",
    "#def change_transformers_dataset_2_right_format(dataset, label_name): \n",
    "   # return dataset.map(lambda example: {'lyric': example[label_name]}, \n",
    "                      # remove_columns=[label_name])\n",
    "\n",
    "#tokenized_datasets = change_transformers_dataset_2_right_format(dataset, 'artist')\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e474fe7-9b88-44ba-90b7-85daec686aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://discuss.huggingface.co/t/keyerror-index-level-0-error-with-datasets-arrow-writer-py/18082/3\n",
    "# Was getting a Keyerror: 0\n",
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497d5a9-ef44-445e-be96-493f46441451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"song\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec447f-9760-47b2-9b93-612f83561ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"album\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa0b76-e0b2-4eb9-bab9-0873f47420d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"artist\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf7116-6a02-495b-a1b1-2587dfece1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"lyric\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f68bb-471b-4494-824a-589ceb12a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"attention_mask\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18e763-8dce-407c-bee2-5ecd94217863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets[\"input_ids\"][564]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71aede4-de32-463c-bc68-2eb4cf2c6415",
   "metadata": {},
   "source": [
    "# train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841090e-364e-4c71-ace9-1ae6539881aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#tokenized_train, tokenized_test = train_test_split(tokenized_datasets, test_size=0.2, \n",
    "                                                  # random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90362c-cb2c-4af8-b1fd-7f2185841256",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n env_pytorch python=3.6\n",
    "!conda activate env_pytorch\n",
    "\n",
    "\n",
    "!torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "726b657f-94d0-4f0c-be3d-5abdd83f4be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: artist, album, lyric, __index_level_0__, song. If artist, album, lyric, __index_level_0__, song are not expected by `GPTNeoForCausalLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 978\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='978' max='978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [978/978 07:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.485200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to tokenized_train/checkpoint-500\n",
      "Configuration saved in tokenized_train/checkpoint-500/config.json\n",
      "Model weights saved in tokenized_train/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in tokenized_train/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in tokenized_train/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=978, training_loss=3.4215069168184433, metrics={'train_runtime': 459.7607, 'train_samples_per_second': 6.382, 'train_steps_per_second': 2.127, 'total_flos': 3065526494429184.0, 'train_loss': 3.4215069168184433, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    #eval_dataset=tokenized_test['input_ids'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,    \n",
    ")\n",
    "\n",
    "trainer = trainer.train()\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb4dca9-4d89-40c2-a373-faf09bc01a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: accelerate: command not found\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fe71db7-2749-4fc8-a511-bbecbc45f71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5054d5d-2dad-4197-8a4b-002c81624028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting koila\n",
      "  Downloading koila-0.1.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from koila) (1.12.0+cu116)\n",
      "Collecting rich\n",
      "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynvml\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from koila) (1.23.1)\n",
      "Collecting pygments<3.0.0,>=2.14.0\n",
      "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.1.0\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->koila) (4.3.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pynvml, pygments, mdurl, markdown-it-py, rich, koila\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.12.0\n",
      "    Uninstalling Pygments-2.12.0:\n",
      "      Successfully uninstalled Pygments-2.12.0\n",
      "Successfully installed koila-0.1.1 markdown-it-py-2.1.0 mdurl-0.1.2 pygments-2.14.0 pynvml-11.4.1 rich-13.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: -c: line 0: syntax error near unexpected token `='\n",
      "/bin/bash: -c: line 0: `(input, label) = lazy(input, label, batch=0)'\n"
     ]
    }
   ],
   "source": [
    "!pip install koila\n",
    "# Wrap the input tensor and label tensor.\n",
    "# If a batch argument is provided, that dimension of the tensor would be treated as the batch.\n",
    "# In this case, the first dimension (dim=0) is used as batch's dimension.\n",
    "\n",
    "!(input, label) = lazy(input, label, batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f6a2-b909-443f-8a37-2df80a69e82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfa8e4c-ad50-4e22-abc6-0090cc323495",
   "metadata": {
    "tags": []
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea4aba4-de39-4770-812a-0ce35b4738d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source https://huggingface.co/course/chapter3/2?fw=pt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754d0820-ec68-42e2-9a59-4d7ad1857cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = load_dataset('csv', data_files='/notebooks/data/2022-10-25_az-lyrics-all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e7bf56-d376-4b81-ba8e-9bd0dbb73ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/notebooks/data/artist_catalogs/CSV/curated_artistdb.csv', nrows=1000)\n",
    "\n",
    "#, nrows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56a473a-6247-42a0-80ee-c297c40a3036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Angels &amp; Demons</td>\n",
       "      <td>What do you see when you're in the dark and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Anomalies</td>\n",
       "      <td>Ayyo\\nYou’re blind to the fact ‘cause you’re a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Beef and Broccoli</td>\n",
       "      <td>Look, let me make something abundantly clear f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Belly of the Beast</td>\n",
       "      <td>This hip-hop verse is somewhat of a change, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Bin Laden</td>\n",
       "      <td>Man, you hear this bullshit they be talking\\nE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Ghostface_Killah</td>\n",
       "      <td>In the Rain (Wise)</td>\n",
       "      <td>I think it happened on a Saturday\\nI heard the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Ghostface_Killah</td>\n",
       "      <td>Ghetto</td>\n",
       "      <td>I was born and raised, in the ghetto\\nI was bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Ghostface_Killah</td>\n",
       "      <td>Love Session</td>\n",
       "      <td>Just like a fresh pair of Royals on the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Ghostface_Killah</td>\n",
       "      <td>Guest House</td>\n",
       "      <td>{We're sorry, the number you have reached is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Ghostface_Killah</td>\n",
       "      <td>Watch ’Em Holla</td>\n",
       "      <td>I've been a lot of places\\nThe very best of ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist          song_title  \\\n",
       "0    Immortal_Technique     Angels & Demons   \n",
       "1    Immortal_Technique           Anomalies   \n",
       "2    Immortal_Technique   Beef and Broccoli   \n",
       "3    Immortal_Technique  Belly of the Beast   \n",
       "4    Immortal_Technique           Bin Laden   \n",
       "..                  ...                 ...   \n",
       "995    Ghostface_Killah  In the Rain (Wise)   \n",
       "996    Ghostface_Killah              Ghetto   \n",
       "997    Ghostface_Killah        Love Session   \n",
       "998    Ghostface_Killah         Guest House   \n",
       "999    Ghostface_Killah     Watch ’Em Holla   \n",
       "\n",
       "                                                lyrics  \n",
       "0    What do you see when you're in the dark and th...  \n",
       "1    Ayyo\\nYou’re blind to the fact ‘cause you’re a...  \n",
       "2    Look, let me make something abundantly clear f...  \n",
       "3    This hip-hop verse is somewhat of a change, so...  \n",
       "4    Man, you hear this bullshit they be talking\\nE...  \n",
       "..                                                 ...  \n",
       "995  I think it happened on a Saturday\\nI heard the...  \n",
       "996  I was born and raised, in the ghetto\\nI was bo...  \n",
       "997  Just like a fresh pair of Royals on the first ...  \n",
       "998  {We're sorry, the number you have reached is n...  \n",
       "999  I've been a lot of places\\nThe very best of ev...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97c2361-3ba7-4ff8-b3eb-2865f37375e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeting the key variables for training\n",
    "df_lyrics = df[['artist','song_title','lyrics']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4ca5d1-c8a5-4833-a190-97fba035025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning dataset where there are no lyrics in a dataset\n",
    "df_lyrics= df_lyrics[df_lyrics['lyrics']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b956637c-27cc-4e62-841e-13402c832d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>994</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Nas</td>\n",
       "      <td>Big Girl</td>\n",
       "      <td>\"What we need to be thinkin' about is the futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>429</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist song_title                                             lyrics\n",
       "count    1000       1000                                               1000\n",
       "unique      6        994                                                998\n",
       "top       Nas   Big Girl  \"What we need to be thinkin' about is the futu...\n",
       "freq      429          2                                                  2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review how many rows have been deleted\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab25bcf-6294-42e2-ba13-c5fee49d6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "df_lyrics= df_lyrics[df_lyrics['lyrics']!=None].drop_duplicates(subset=['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369e4f7e-7fa0-4e5c-aac1-f4206891d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with null values\n",
    "df_lyrics = df_lyrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dc774a-5807-4829-ab27-aaafd5393ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>406</td>\n",
       "      <td>247</td>\n",
       "      <td>966</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Chris Webby</td>\n",
       "      <td>RICANstruction: The Black Rosary</td>\n",
       "      <td>Trouble</td>\n",
       "      <td>My lil homie caught a\\nMy lil homie caught a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>90</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                             album     song  \\\n",
       "count           978                               978      978   \n",
       "unique          406                               247      966   \n",
       "top     Chris Webby  RICANstruction: The Black Rosary  Trouble   \n",
       "freq             90                                28        2   \n",
       "\n",
       "                                                    lyric  \n",
       "count                                                 978  \n",
       "unique                                                978  \n",
       "top     My lil homie caught a\\nMy lil homie caught a h...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see how many rows have been deleted after cleaning\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cdb6865-fab5-4e73-b153-68f0e95a69b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist                                                      Nas\n",
       "song_title                               Eye 4 An Eye Freestyle\n",
       "lyrics        Yo this Nas niggas whuttup?\\nQB Album coming, ...\n",
       "Name: 564, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics.iloc[564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55df0e73-3aa0-4caf-a634-f854eb447b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo this Nas niggas whuttup?\\nQB Album coming, niggas Ill Will, Bravehearts\\nMy nigga DJ Clue - put this shit on these niggas\\nDesert Storm baby 2000!\\nA Drug dealers dream - flip his last Ki out the game\\nCause entertainment money now is off the chain\\nI put blunts in niggas caskets, lost in the game\\nI learned about pain, bodies burnt in drug game\\nBut I'm neva gon' die, I neva heard of death\\nEnergy could neva be destroyed\\nOnly the flesh, when you niggas try to murder me\\nWith bullets to my head\\nThis is my you can't kill me niggas\\nI'm already dead, BRING IT!!\\nI'm busting bodies fall in the cemetery\\nBlack blood raining on the street\\nAll you niggas, buried y'all\\nNiggas just came home, fresh out the state greens\\nFive time felon, thinking he jelling in Queens\\nIt's pitiful - what bid could do, especially federal\\nY'all young thugs wilding, see prison\\nGot a bed for you waiting, I clapped that Satan\\nBut thought I was dreaming\\nI woke up masturbating in the bed with demons\\nI cried with every bit of strength of my small body\\nThis is the life I chose, under God I'm tiny\\nHard to find me, I popped up, lock shot-up\\nYou can't B.I., see I'm eternal, not luck\\nYou get shot-up, boxed-up next for the grave\\nYour flow is one dimensional, your level is 2nd grade\\nYou're on top -- WHAT?\\nCopy and fuck, I said it first, you repeated\\nYour false crown covered in dirt - defeated\\nY'all niggas all hail, the King is dead\\nHe running like a bitch with his tail between his legs\\n'Stillmatic', still eye 4 an eye, wanna be God\\nYou're just the next rapper to die, fucking with Nas\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics[\"lyrics\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9796cd4c-7c89-4174-9fe3-0728a6b6e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source - https://huggingface.co/docs/datasets/loading#pandas-dataframe\n",
    "#Source - https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/loading_methods#datasets.load_dataset.split\n",
    "#maybe we need to split the data source during loading\n",
    "dataset = Dataset.from_pandas(df_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667a78c-e103-430f-94db-c762ce0bc18a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203d19ec-05c6-484e-b1c5-1245dfd4e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d3af91-23db-4172-bec0-74422dbdc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'song_title': Value(dtype='string', id=None),\n",
       " 'lyrics': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ce2b94-3f31-4c5b-80e1-a388d751e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.4.0)\n",
      "Collecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.23.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: dill, multiprocess\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.13\n",
      "    Uninstalling multiprocess-0.70.13:\n",
      "      Successfully uninstalled multiprocess-0.70.13\n",
      "Successfully installed dill-0.3.4 multiprocess-0.70.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#$ pip3 install dill\n",
    "#source https://github.com/huggingface/datasets/issues/4506#issuecomment-1157417219\n",
    "!pip install datasets \"dill<0.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95505e5d-699e-4b3a-a1c9-e939a282a673",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50130ae8-e0ed-43d3-b66b-de15dcb97715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a558f325c9fb4655bc3c59a05997dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55469f3fb5964460be931cf1792c23b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9bce9c3e504b1cabd06d5dc73856d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab823d27bed84a0c904b724c240e70e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d562aa574449cfbcca421e3690fff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Source - using tutorial https://huggingface.co/docs/transformers/training\n",
    "#Source - tokenizer with targeted model https://huggingface.co/EleutherAI/gpt-neo-125M\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c0b3e-c34e-44ec-9b4b-d9b48bf78429",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa766478-bde1-40b2-b4b2-d9a102173cf2",
   "metadata": {},
   "source": [
    "tokenizer.pad_token_id\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa38ddf8-7f6d-4d51-bb58-56c5f35463de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://github.com/huggingface/transformers/issues/2630#issuecomment-1290809338\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592347dd-2821-401c-93e3-1356489264b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334b46d004c648febaa7d3416f011f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['artist', 'song_title', 'lyrics', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 998\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Source - using tutorial https://huggingface.co/docs/transformers/training\n",
    "#first have to tokenize custom dataset\n",
    "#source for below quote: https://huggingface.co/course/chapter3/2?fw=pt\n",
    "#\"Note that we’ve left the padding argument out in our tokenization function for now. \n",
    "#This is because padding all the samples to the maximum length is not efficient: \n",
    "#it’s better to pad the samples when we’re building a batch, as then we only need to pad to the \n",
    "#maximum length in that batch, and not the maximum length in the entire dataset. \n",
    "#This can save a lot of time and processing power when the inputs have very variable lengths!\"\n",
    "\n",
    "\n",
    "#padding and trucation concepts https://huggingface.co/docs/transformers/v4.25.1/en/pad_truncation\n",
    "def tokenize_function(element):\n",
    "    return tokenizer(element[\"lyrics\"], truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "##padding=\"max_length\",\n",
    "#max_length=context_length,\n",
    "#we took out the padding because we will use dynamic padding later which is more efficient\n",
    "#padding=\"longest\",\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2471f7d-3f39-48ca-840d-ceb77c63ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 1024\n",
      "Input IDs length: (1024,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "outputs = tokenized_datasets[1]\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input IDs length: {np.array(outputs['input_ids']).shape}\")\n",
    "#print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "#print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b39cba3-51e9-4753-b690-5fbe065a0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://huggingface.co/course/chapter7/6?fw=pt#initializing-a-new-model\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    checkpoint,\n",
    "    vocab_size=len(tokenizer),\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349a9172-4ee9-4a5b-acc7-e0e10d376d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist', 'song_title', 'lyrics', '__index_level_0__', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a3ef7a6-c7ba-45ef-b057-fc43cbe5b68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"attention_mask\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52293d41-94ca-4d20-b515-85f8db4b3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets[\"input_ids\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6c32b0-ef93-48ba-aa72-a0b9e2c802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://huggingface.co/docs/transformers/training\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "010f8d80-f29c-444b-b329-fa69467cad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://huggingface.co/course/chapter7/6?fw=pt\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46eabe26-d1e8-4145-9d11-689f9253b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step is to define model\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5bde73-b7cd-4e99-a4b0-3200b3b21ac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb72bb14-3a8f-4c83-950d-589810b2e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 1\n",
    "#from transformers import TrainingArguments\n",
    "#training_args = TrainingArguments(\"tokenized_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d545a6d-9fca-419f-bb8d-72c9a91a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2 customize all the training arguments\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "   \"tokenized_train\",\n",
    "   per_device_train_batch_size=3,\n",
    "   per_device_eval_batch_size=3,\n",
    "   num_train_epochs=3,\n",
    "   learning_rate=2e-5,\n",
    "   weight_decay=0.01,\n",
    "    fp16=True\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59776d-3f09-4983-926f-e4d0ab545eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# clean tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fe1bd9e-e213-414d-991e-04e97d5d6698",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (800526342.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [30]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"__index_level_0__\"])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"song\", \"album\", \"artist\", \"lyric\", \n",
    "                                                       # \"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db459d87-e778-46e9-a3d9-e6c9a3ef9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba2519-f74d-463f-96df-5d605ae09e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source https://github.com/huggingface/transformers/issues/9636#issuecomment-761571219\n",
    "#following code gets error becase dataset has not lable variable - Keyerror: 5\n",
    "\n",
    "#def change_transformers_dataset_2_right_format(dataset, label_name): \n",
    "   # return dataset.map(lambda example: {'lyric': example[label_name]}, \n",
    "                      # remove_columns=[label_name])\n",
    "\n",
    "#tokenized_datasets = change_transformers_dataset_2_right_format(dataset, 'artist')\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e474fe7-9b88-44ba-90b7-85daec686aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source https://discuss.huggingface.co/t/keyerror-index-level-0-error-with-datasets-arrow-writer-py/18082/3\n",
    "# Was getting a Keyerror: 0\n",
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "#tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71aede4-de32-463c-bc68-2eb4cf2c6415",
   "metadata": {},
   "source": [
    "# train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841090e-364e-4c71-ace9-1ae6539881aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#tokenized_train, tokenized_test = train_test_split(tokenized_datasets, test_size=0.2, \n",
    "                                                  # random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90362c-cb2c-4af8-b1fd-7f2185841256",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n env_pytorch python=3.6\n",
    "!conda activate env_pytorch\n",
    "\n",
    "\n",
    "!torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "726b657f-94d0-4f0c-be3d-5abdd83f4be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: artist, album, lyric, __index_level_0__, song. If artist, album, lyric, __index_level_0__, song are not expected by `GPTNeoForCausalLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 978\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='978' max='978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [978/978 07:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.485200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to tokenized_train/checkpoint-500\n",
      "Configuration saved in tokenized_train/checkpoint-500/config.json\n",
      "Model weights saved in tokenized_train/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in tokenized_train/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in tokenized_train/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=978, training_loss=3.4215069168184433, metrics={'train_runtime': 459.7607, 'train_samples_per_second': 6.382, 'train_steps_per_second': 2.127, 'total_flos': 3065526494429184.0, 'train_loss': 3.4215069168184433, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    #eval_dataset=tokenized_test['input_ids'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,    \n",
    ")\n",
    "\n",
    "trainer = trainer.train()\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb4dca9-4d89-40c2-a373-faf09bc01a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: accelerate: command not found\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fe71db7-2749-4fc8-a511-bbecbc45f71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5054d5d-2dad-4197-8a4b-002c81624028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting koila\n",
      "  Downloading koila-0.1.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from koila) (1.12.0+cu116)\n",
      "Collecting rich\n",
      "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynvml\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from koila) (1.23.1)\n",
      "Collecting pygments<3.0.0,>=2.14.0\n",
      "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.1.0\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->koila) (4.3.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pynvml, pygments, mdurl, markdown-it-py, rich, koila\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.12.0\n",
      "    Uninstalling Pygments-2.12.0:\n",
      "      Successfully uninstalled Pygments-2.12.0\n",
      "Successfully installed koila-0.1.1 markdown-it-py-2.1.0 mdurl-0.1.2 pygments-2.14.0 pynvml-11.4.1 rich-13.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: -c: line 0: syntax error near unexpected token `='\n",
      "/bin/bash: -c: line 0: `(input, label) = lazy(input, label, batch=0)'\n"
     ]
    }
   ],
   "source": [
    "!pip install koila\n",
    "# Wrap the input tensor and label tensor.\n",
    "# If a batch argument is provided, that dimension of the tensor would be treated as the batch.\n",
    "# In this case, the first dimension (dim=0) is used as batch's dimension.\n",
    "\n",
    "!(input, label) = lazy(input, label, batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f6a2-b909-443f-8a37-2df80a69e82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

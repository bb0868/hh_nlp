{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a63344f-a2f9-4ae9-a361-812e698436af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c2e002-38c8-452d-8052-d88975ad426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd091aa-4491-4f6e-b526-481748b4783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/notebooks/data/artist_catalogs/CSV/curated_artistdb.csv')\n",
    "\n",
    "#, nrows=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7ae7d4-6988-49f7-91e6-7126a9e37fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Angels &amp; Demons</td>\n",
       "      <td>What do you see when you're in the dark and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Anomalies</td>\n",
       "      <td>Ayyo\\nYou’re blind to the fact ‘cause you’re a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Beef and Broccoli</td>\n",
       "      <td>Look, let me make something abundantly clear f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Belly of the Beast</td>\n",
       "      <td>This hip-hop verse is somewhat of a change, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immortal_Technique</td>\n",
       "      <td>Bin Laden</td>\n",
       "      <td>Man, you hear this bullshit they be talking\\nE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>Army_of_the_Pharaohs</td>\n",
       "      <td>War Ensemble</td>\n",
       "      <td>\"In a war there are many moments for compassio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10417</th>\n",
       "      <td>Army_of_the_Pharaohs</td>\n",
       "      <td>War Machine</td>\n",
       "      <td>Mutilate massacre mummies that's comin' to mai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10418</th>\n",
       "      <td>Army_of_the_Pharaohs</td>\n",
       "      <td>Warth of Gods</td>\n",
       "      <td>I could talk bitches out of they jeans\\nGold d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10419</th>\n",
       "      <td>Army_of_the_Pharaohs</td>\n",
       "      <td>Wrath of Gods</td>\n",
       "      <td>I could talk bitches out of they jeans\\nGold d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10420</th>\n",
       "      <td>Army_of_the_Pharaohs</td>\n",
       "      <td>Wrath Prophecy</td>\n",
       "      <td>Motherfuckers always wanna talk slick, man\\nTi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10421 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     artist          song_title  \\\n",
       "0        Immortal_Technique     Angels & Demons   \n",
       "1        Immortal_Technique           Anomalies   \n",
       "2        Immortal_Technique   Beef and Broccoli   \n",
       "3        Immortal_Technique  Belly of the Beast   \n",
       "4        Immortal_Technique           Bin Laden   \n",
       "...                     ...                 ...   \n",
       "10416  Army_of_the_Pharaohs        War Ensemble   \n",
       "10417  Army_of_the_Pharaohs         War Machine   \n",
       "10418  Army_of_the_Pharaohs       Warth of Gods   \n",
       "10419  Army_of_the_Pharaohs       Wrath of Gods   \n",
       "10420  Army_of_the_Pharaohs      Wrath Prophecy   \n",
       "\n",
       "                                                  lyrics  \n",
       "0      What do you see when you're in the dark and th...  \n",
       "1      Ayyo\\nYou’re blind to the fact ‘cause you’re a...  \n",
       "2      Look, let me make something abundantly clear f...  \n",
       "3      This hip-hop verse is somewhat of a change, so...  \n",
       "4      Man, you hear this bullshit they be talking\\nE...  \n",
       "...                                                  ...  \n",
       "10416  \"In a war there are many moments for compassio...  \n",
       "10417  Mutilate massacre mummies that's comin' to mai...  \n",
       "10418  I could talk bitches out of they jeans\\nGold d...  \n",
       "10419  I could talk bitches out of they jeans\\nGold d...  \n",
       "10420  Motherfuckers always wanna talk slick, man\\nTi...  \n",
       "\n",
       "[10421 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e38e128-95c7-4843-8e0c-38e069681fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeting the key variables for training\n",
    "df_lyrics = df[['artist','song_title','lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4c23a3-7c9d-4507-83fe-fe5e532082d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning dataset where there are no lyrics in a dataset\n",
    "df_lyrics= df_lyrics[df_lyrics['lyrics']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2a4ef3-8bbb-48a3-844a-31c0ddfa54eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10421</td>\n",
       "      <td>10419</td>\n",
       "      <td>10421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>79</td>\n",
       "      <td>9900</td>\n",
       "      <td>10260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Nas</td>\n",
       "      <td>Intro</td>\n",
       "      <td>Girl you know that you need\\nTo stop givin' me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>429</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist song_title                                             lyrics\n",
       "count   10421      10419                                              10421\n",
       "unique     79       9900                                              10260\n",
       "top       Nas      Intro  Girl you know that you need\\nTo stop givin' me...\n",
       "freq      429         26                                                 12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review how many rows have been deleted\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0beda46-0b4e-4237-b81f-9463282a970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "df_lyrics= df_lyrics[df_lyrics['lyrics']!=None].drop_duplicates(subset=['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f75896b-44a8-4d50-a8d0-fdb5b134742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with null values\n",
    "df_lyrics = df_lyrics.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d578a886-67cf-4bbb-b04b-910e46bcec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10258</td>\n",
       "      <td>10258</td>\n",
       "      <td>10258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>78</td>\n",
       "      <td>9781</td>\n",
       "      <td>10258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Nas</td>\n",
       "      <td>Intro</td>\n",
       "      <td>What do you see when you're in the dark and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>427</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist song_title                                             lyrics\n",
       "count   10258      10258                                              10258\n",
       "unique     78       9781                                              10258\n",
       "top       Nas      Intro  What do you see when you're in the dark and th...\n",
       "freq      427         26                                                  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see how many rows have been deleted after cleaning\n",
    "df_lyrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a189166b-6341-4fb2-aa56-56cc3a07a21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist                                                      Nas\n",
       "song_title                               Eye 4 An Eye Freestyle\n",
       "lyrics        Yo this Nas niggas whuttup?\\nQB Album coming, ...\n",
       "Name: 564, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics.iloc[564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b21fd7-8b58-4f20-8c57-e9bc8b1cca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo this Nas niggas whuttup?\\nQB Album coming, niggas Ill Will, Bravehearts\\nMy nigga DJ Clue - put this shit on these niggas\\nDesert Storm baby 2000!\\nA Drug dealers dream - flip his last Ki out the game\\nCause entertainment money now is off the chain\\nI put blunts in niggas caskets, lost in the game\\nI learned about pain, bodies burnt in drug game\\nBut I'm neva gon' die, I neva heard of death\\nEnergy could neva be destroyed\\nOnly the flesh, when you niggas try to murder me\\nWith bullets to my head\\nThis is my you can't kill me niggas\\nI'm already dead, BRING IT!!\\nI'm busting bodies fall in the cemetery\\nBlack blood raining on the street\\nAll you niggas, buried y'all\\nNiggas just came home, fresh out the state greens\\nFive time felon, thinking he jelling in Queens\\nIt's pitiful - what bid could do, especially federal\\nY'all young thugs wilding, see prison\\nGot a bed for you waiting, I clapped that Satan\\nBut thought I was dreaming\\nI woke up masturbating in the bed with demons\\nI cried with every bit of strength of my small body\\nThis is the life I chose, under God I'm tiny\\nHard to find me, I popped up, lock shot-up\\nYou can't B.I., see I'm eternal, not luck\\nYou get shot-up, boxed-up next for the grave\\nYour flow is one dimensional, your level is 2nd grade\\nYou're on top -- WHAT?\\nCopy and fuck, I said it first, you repeated\\nYour false crown covered in dirt - defeated\\nY'all niggas all hail, the King is dead\\nHe running like a bitch with his tail between his legs\\n'Stillmatic', still eye 4 an eye, wanna be God\\nYou're just the next rapper to die, fucking with Nas\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics[\"lyrics\"][564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d413b0f-2969-43d9-9547-1b44b122cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source - https://huggingface.co/docs/datasets/loading#pandas-dataframe\n",
    "#Source - https://huggingface.co/docs/datasets/v2.9.0/en/package_reference/loading_methods#datasets.load_dataset.split\n",
    "#maybe we need to split the data source during loading\n",
    "dataset = Dataset.from_pandas(df_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e63d4a9-4358-4b6e-a735-6e5c0adb12bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'song_title': Value(dtype='string', id=None),\n",
       " 'lyrics': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921ddefe-eb9d-4c19-b800-b66cde5e27cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': Value(dtype='string', id=None),\n",
       " 'lyrics': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"song_title\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9821ab-bee7-4d28-aeff-b61753659114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lyrics': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"artist\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b64da73-038e-46f0-b83c-681761be754b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lyrics': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e002c64-b147-4ad3-a806-34f67c92eccd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19375381-2cbe-4360-9658-bdadca985dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ds_train, ds_test = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaee7c0-ba32-4715-8041-43ca227ed81d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0fe51b5-c30c-4290-988b-101f8467bad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b279fd83360e495d9be395d1dc7ae96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af977093cff4451b91ab94aaa4686c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3881ca8b10324834968c609ec6e65adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40047deeb09e4c8da11d665cab733dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 4\n",
      "Input chunk lengths: [500, 197, 500, 269]\n",
      "Chunk mapping: [0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "context_length = 500\n",
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = tokenizer(\n",
    "    dataset[\"lyrics\"][:2],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdbc6e-fd3f-4fdd-ae1c-7519e3f6a1c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tokenize def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4032202-f9df-4d32-a3c8-dc2dbc44f511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438b6067599f44ec853d79f75d2ca216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 10962\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"lyrics\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=[\"lyrics\"])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e316ba55-d4c9-47a1-b29c-880f2556581e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3855,\n",
       "  12431,\n",
       "  44873,\n",
       "  4908,\n",
       "  198,\n",
       "  6,\n",
       "  42323,\n",
       "  314,\n",
       "  1101,\n",
       "  64,\n",
       "  651,\n",
       "  6164,\n",
       "  198,\n",
       "  818,\n",
       "  2233,\n",
       "  640,\n",
       "  198,\n",
       "  28172,\n",
       "  17753,\n",
       "  18340,\n",
       "  198,\n",
       "  3855,\n",
       "  12431,\n",
       "  11,\n",
       "  314,\n",
       "  11662,\n",
       "  299,\n",
       "  1127,\n",
       "  198,\n",
       "  40,\n",
       "  910,\n",
       "  428,\n",
       "  530,\n",
       "  640,\n",
       "  198,\n",
       "  464,\n",
       "  938,\n",
       "  640,\n",
       "  11,\n",
       "  314,\n",
       "  21192,\n",
       "  284,\n",
       "  331,\n",
       "  6,\n",
       "  439,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  422,\n",
       "  810,\n",
       "  198,\n",
       "  464,\n",
       "  45654,\n",
       "  25912,\n",
       "  2412,\n",
       "  318,\n",
       "  379,\n",
       "  198,\n",
       "  1870,\n",
       "  25542,\n",
       "  27318,\n",
       "  198,\n",
       "  1870,\n",
       "  3013,\n",
       "  9249,\n",
       "  869,\n",
       "  1644,\n",
       "  198,\n",
       "  2202,\n",
       "  777,\n",
       "  299,\n",
       "  6950,\n",
       "  292,\n",
       "  11,\n",
       "  290,\n",
       "  4675,\n",
       "  198,\n",
       "  41389,\n",
       "  364,\n",
       "  7288,\n",
       "  338,\n",
       "  588,\n",
       "  198,\n",
       "  7556,\n",
       "  36749,\n",
       "  6,\n",
       "  5727,\n",
       "  290,\n",
       "  198,\n",
       "  16501,\n",
       "  1648,\n",
       "  259,\n",
       "  6,\n",
       "  299,\n",
       "  6950,\n",
       "  292,\n",
       "  287,\n",
       "  6698,\n",
       "  290,\n",
       "  923,\n",
       "  259,\n",
       "  6,\n",
       "  10512,\n",
       "  290,\n",
       "  198,\n",
       "  2484,\n",
       "  1025,\n",
       "  259,\n",
       "  6,\n",
       "  299,\n",
       "  6950,\n",
       "  292,\n",
       "  11,\n",
       "  4692,\n",
       "  2910,\n",
       "  11,\n",
       "  655,\n",
       "  1494,\n",
       "  705,\n",
       "  368,\n",
       "  11,\n",
       "  523,\n",
       "  644,\n",
       "  198,\n",
       "  3152,\n",
       "  262,\n",
       "  11511,\n",
       "  6,\n",
       "  14896,\n",
       "  467,\n",
       "  14380,\n",
       "  198,\n",
       "  28971,\n",
       "  3105,\n",
       "  510,\n",
       "  198,\n",
       "  8421,\n",
       "  345,\n",
       "  651,\n",
       "  534,\n",
       "  840,\n",
       "  384,\n",
       "  675,\n",
       "  510,\n",
       "  198,\n",
       "  45,\n",
       "  328,\n",
       "  4908,\n",
       "  1239,\n",
       "  35140,\n",
       "  6,\n",
       "  1663,\n",
       "  510,\n",
       "  198,\n",
       "  6187,\n",
       "  676,\n",
       "  259,\n",
       "  6,\n",
       "  262,\n",
       "  367,\n",
       "  11870,\n",
       "  198,\n",
       "  1537,\n",
       "  339,\n",
       "  760,\n",
       "  339,\n",
       "  35140,\n",
       "  6,\n",
       "  3714,\n",
       "  510,\n",
       "  198,\n",
       "  23433,\n",
       "  259,\n",
       "  6,\n",
       "  284,\n",
       "  787,\n",
       "  8469,\n",
       "  4200,\n",
       "  198,\n",
       "  2061,\n",
       "  340,\n",
       "  804,\n",
       "  588,\n",
       "  30,\n",
       "  10898,\n",
       "  329,\n",
       "  257,\n",
       "  2042,\n",
       "  4257,\n",
       "  198,\n",
       "  3987,\n",
       "  470,\n",
       "  765,\n",
       "  284,\n",
       "  787,\n",
       "  616,\n",
       "  1995,\n",
       "  2611,\n",
       "  3960,\n",
       "  198,\n",
       "  2202,\n",
       "  257,\n",
       "  2166,\n",
       "  2443,\n",
       "  329,\n",
       "  257,\n",
       "  19625,\n",
       "  198,\n",
       "  33771,\n",
       "  348,\n",
       "  2419,\n",
       "  11,\n",
       "  477,\n",
       "  314,\n",
       "  2227,\n",
       "  284,\n",
       "  3708,\n",
       "  198,\n",
       "  8496,\n",
       "  43803,\n",
       "  651,\n",
       "  698,\n",
       "  403,\n",
       "  1513,\n",
       "  290,\n",
       "  6594,\n",
       "  198,\n",
       "  5211,\n",
       "  13343,\n",
       "  287,\n",
       "  262,\n",
       "  3049,\n",
       "  11193,\n",
       "  198,\n",
       "  1870,\n",
       "  257,\n",
       "  3049,\n",
       "  8761,\n",
       "  11,\n",
       "  442,\n",
       "  47337,\n",
       "  6,\n",
       "  262,\n",
       "  279,\n",
       "  7916,\n",
       "  198,\n",
       "  4561,\n",
       "  437,\n",
       "  259,\n",
       "  6,\n",
       "  12014,\n",
       "  319,\n",
       "  1263,\n",
       "  14659,\n",
       "  198,\n",
       "  1870,\n",
       "  262,\n",
       "  288,\n",
       "  999,\n",
       "  326,\n",
       "  1577,\n",
       "  14290,\n",
       "  198,\n",
       "  2504,\n",
       "  373,\n",
       "  257,\n",
       "  4019,\n",
       "  338,\n",
       "  290,\n",
       "  4101,\n",
       "  338,\n",
       "  1517,\n",
       "  198,\n",
       "  3844,\n",
       "  314,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  45,\n",
       "  993,\n",
       "  11,\n",
       "  314,\n",
       "  18959,\n",
       "  470,\n",
       "  46733,\n",
       "  6,\n",
       "  351,\n",
       "  345,\n",
       "  198,\n",
       "  3844,\n",
       "  314,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  38,\n",
       "  12375,\n",
       "  651,\n",
       "  616,\n",
       "  1204,\n",
       "  1978,\n",
       "  582,\n",
       "  198,\n",
       "  6,\n",
       "  42323,\n",
       "  783,\n",
       "  314,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  45,\n",
       "  993,\n",
       "  11,\n",
       "  314,\n",
       "  18959,\n",
       "  470,\n",
       "  35140,\n",
       "  6,\n",
       "  6594,\n",
       "  351,\n",
       "  345,\n",
       "  198,\n",
       "  6,\n",
       "  42323,\n",
       "  783,\n",
       "  314,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  45,\n",
       "  993,\n",
       "  11,\n",
       "  314,\n",
       "  460,\n",
       "  470,\n",
       "  466,\n",
       "  326,\n",
       "  7510,\n",
       "  645,\n",
       "  517,\n",
       "  198,\n",
       "  3844,\n",
       "  314,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  45,\n",
       "  993,\n",
       "  11,\n",
       "  314,\n",
       "  460,\n",
       "  470,\n",
       "  5089,\n",
       "  351,\n",
       "  345,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  45,\n",
       "  993,\n",
       "  11,\n",
       "  314,\n",
       "  18959,\n",
       "  470,\n",
       "  35140,\n",
       "  6,\n",
       "  1302,\n",
       "  319,\n",
       "  645,\n",
       "  14371,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  51,\n",
       "  1202,\n",
       "  286,\n",
       "  1561,\n",
       "  259,\n",
       "  6,\n",
       "  1402,\n",
       "  1561,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  40,\n",
       "  1101,\n",
       "  656,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  464,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  464,\n",
       "  1263,\n",
       "  1243,\n",
       "  198,\n",
       "  2215,\n",
       "  314,\n",
       "  2067,\n",
       "  428,\n",
       "  198,\n",
       "  3237,\n",
       "  314,\n",
       "  2227,\n",
       "  373,\n",
       "  5006,\n",
       "  290,\n",
       "  48083,\n",
       "  82,\n",
       "  198,\n",
       "  7594,\n",
       "  584,\n",
       "  4095,\n",
       "  7912,\n",
       "  475,\n",
       "  198,\n",
       "  5211,\n",
       "  314,\n",
       "  760,\n",
       "  810,\n",
       "  314,\n",
       "  1101,\n",
       "  467,\n",
       "  259,\n",
       "  6,\n",
       "  284,\n",
       "  30,\n",
       "  198,\n",
       "  6090,\n",
       "  314,\n",
       "  6044,\n",
       "  618,\n",
       "  314,\n",
       "  1625,\n",
       "  422,\n",
       "  30,\n",
       "  198,\n",
       "  40,\n",
       "  307,\n",
       "  905,\n",
       "  259,\n",
       "  6,\n",
       "  345,\n",
       "  198,\n",
       "  6610,\n",
       "  3918,\n",
       "  314,\n",
       "  1101,\n",
       "  64,\n",
       "  5202,\n",
       "  329,\n",
       "  345,\n",
       "  198,\n",
       "  6090,\n",
       "  314,\n",
       "  787,\n",
       "  1054,\n",
       "  20079,\n",
       "  6,\n",
       "  4295,\n",
       "  198,\n",
       "  40,\n",
       "  460,\n",
       "  3714,\n",
       "  329,\n",
       "  345,\n",
       "  11,\n",
       "  5202,\n",
       "  329,\n",
       "  345,\n",
       "  198,\n",
       "  12050,\n",
       "  340,\n",
       "  3182,\n",
       "  12,\n",
       "  1073,\n",
       "  459]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[564]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680dd90-6f09-4f40-938a-2520d5631fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c24778c4-cf4d-4c8f-a065-1d2dd6fa343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    checkpoint,\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247179c7-3d8f-410f-ad78-02129ca26544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665956f3ac404287bae6d66c24098903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d72eb9-24af-4033-8cc4-ae7ba21ff44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea811e0e-1124-48d9-b8b2-9c47c052f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5e0b546-ab5a-4a06-b4fc-b2626dd3a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 500])\n",
      "attention_mask shape: torch.Size([5, 500])\n",
      "labels shape: torch.Size([5, 500])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8172a129-2254-4ddb-8837-bfce0cbdc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [tokenized_datasets[i] for i in range(8)]\n",
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf0c26-0d62-4999-b03d-bb9cf8acdeb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# login into HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d7d3a46-00cd-4a04-ab65-17a742f36400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c075a2f004cacbfd628a12ae2ad7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29811cef-d89a-4d48-8bd5-b83ef94fcfe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# split tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce960e06-0355-4770-9bdc-d343d81ad3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ds_train, ds_test = train_test_split(tokenized_datasets, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4885bcf8-d1ec-4e00-9745-152b63815712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.0.9\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n"
     ]
    }
   ],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c747d532-3a30-4851-8672-594626edbefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 74 not upgraded.\n",
      "Need to get 7419 kB of archives.\n",
      "After this operation, 16.0 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.3.0 [7419 kB]\n",
      "Fetched 7419 kB in 1s (11.4 MB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 69943 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.3.0_amd64.deb ...\n",
      "Unpacking git-lfs (3.3.0) ...\n",
      "Setting up git-lfs (3.3.0) ...\n",
      "Git LFS initialized.\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ac34b-1c35-46d4-a895-8ea226af729f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Aruguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "897110f2-ff04-4d44-b4fc-dd9b431fffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'private'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiphop-ds-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#eval_dataset=ds_test,\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:482\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py:3244\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m get_full_repo_name(repo_name, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token)\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moverwrite_output_dir \u001b[38;5;129;01mand\u001b[39;00m at_init:\n\u001b[1;32m   3252\u001b[0m         \u001b[38;5;66;03m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'private'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"hiphop-ds-v3\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    #eval_dataset=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc709a7-1294-4de1-901c-fa7f98364757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1735\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37/60 20:27 < 13:26, 0.03 it/s, Epoch 5.88/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe4d1a-2124-4673-afc8-3aa82a3aa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c6fcb-0ae0-42e6-a9ad-96bc61018261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
